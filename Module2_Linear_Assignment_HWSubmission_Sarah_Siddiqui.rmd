---
title: "Module 2 Assignment on Linear Regression - 2"
author: "Sarah Siddiqui // Graduate Student"
date: "02/24/2021"
output:
  pdf_document: default
  df_print: paged

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, tidy=TRUE, warning = FALSE, tidy.opts=list(width.cutoff=80))
```



## Q1) *Forward and Backward Selection*
```{r eval=FALSE, echo=FALSE, results='hide'}
#This is setup to start
library(ISLR)
Model_3 = mpg ~ horsepower+year+cylinders+displacement+weight+acceleration
Model_3.fit = lm(Model_3, data=Auto)
summary(Model_3.fit)
# Or, prefer this restructuring way
# by excluding categorical variables:
# Make sure AutoNum is a data.frame
AutoNum = Auto[, !(colnames(Auto) %in% c("origin", "name"))]
Model_Full = mpg ~ . #you can write models in this way to call later
Model_Full.fit = lm(Model_Full, data=AutoNum)
summary(Model_Full.fit)

```

*Part a: Model fitted with all predictors*
```{r results='hide'}
library(ISLR)
AutoNum = Auto[, !(colnames(Auto) %in% c("origin", "name"))]
Fullmodel = mpg ~ . #you can write models in this way to call later
Fullmodel.fit = lm(Fullmodel, data=AutoNum)
coeff<-Fullmodel.fit$coefficients
summary(Fullmodel.fit)
r2adj<-0.8063
mse<-mean(Fullmodel.fit$residuals^2)
```
$Coefficients:$
```{r}
coeff
```
$R^2_{adj}:$
```{r}
r2adj
```
$Mean Squared Error:$
```{r}
mse
```

***

*Part b: Forward Selection MLR*

```{r results='hide'}
library(leaps)
regfit.m1=regsubsets(mpg~., data=AutoNum, nbest=1, nvmax=6, method="forward")
reg.summary=summary(regfit.m1)
reg.summary
names(reg.summary)
reg.summary$adjr2
coef(regfit.m1, 1:6) #coefficients of all models built
coefff<-coef(regfit.m1, 2)
r2adjf<-reg.summary$adjr2[2]
sse<-regfit.m1$rss[2]
msef<-sse/392-(7+1)  #sse/n-(p+1)
```
The best model obtained is Model 2, with predictors *Weight* and *Year*.  
Coefficients of model:
```{r}
coefff
```
Adjusted R-squared, $R^2_{adj}:$
```{r}
r2adjf
```

Mean Squared Error:

```{r}
msef

```

***
*Part c: Criteria used*  

The best subset was chosen based on the *forward stepwise selection* method. Here we started with one predictor then iteratively added more predictors until the best model is obtained, defined as the one with the highest $R^2{adj}$ or lowest SSE.  

Other methods are *backward stepwise selection* (starting with the full model then reducing predictors), *best subset* method (best model from all $2^p$ possibilities) and the *hybrid* approach comibining forward and backward approach.


***
*Part d: Backward Selection*  
```{r results='hide'}
regfit.m2=regsubsets(mpg~., data=AutoNum, nbest=1, nvmax=6, method="backward")
reg.summaryb=summary(regfit.m2)
reg.summaryb
reg.summaryb$adjr2
coef(regfit.m2, 1:6) #coefficients of all models built
coeffb<-coef(regfit.m1, 2)
r2adjb<-reg.summary$adjr2[2]
sseb<-regfit.m2$rss[2]
mseb<-sseb/392-(7+1)  #sse/n-(p+1)
```

The best model obtained is Model 2, with predictors *Weight* and *Year*.  
Coefficients of model:
```{r}
coeffb
```
Adjusted R-squared, $R^2_{adj}:$
```{r}
r2adjb
```

Mean Squared Error:

```{r}
mseb
```


***
*Part e: Comparing OLS, Forward and Backward*  


Forward and Backward have the same adjusted R-squared and MSE values, which are respectively higher and lower than the values in OLS, making either of the subset models better than ordinary least squared. The similar results for forward and backward make sense because there's a small number of predictors.

***



\newpage

## Q2) Cross-Validated with k-Fold

*Part a: OLS MLR on mpg*  


```{r results='hide'}
#function
predict.regsubsets=function(object, newdata, id, ...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi #prediction or fitted results
}

AutoNum = Auto[, !(colnames(Auto) %in% c("origin", "name"))]
Model_Full = mpg ~ . #you can write models in this way to call later
Model_Full.fit = lm(Model_Full, data=AutoNum)
k=5 #you will need k=5 in the assignment
set.seed(99)
folds=sample(1:k,nrow(AutoNum),replace=TRUE)
folds
mse.storage=matrix(NA,k,2) #for k-fold, with no model selection
mse.storage
for(j in 1:k){
  # use (k-1)-fold data with all predictors
  fit=lm(mpg~.,data=AutoNum[folds!=j,])
  mse_train = mean(residuals(fit)^2)
  pred=predict(fit,AutoNum[folds==j,])
  mse_test = mean( (AutoNum$mpg[folds==j]-pred)^2)
  mse.storage[j,] = cbind(mse_train, mse_test)
}
mse.storage
apply(mse.storage, 2, mean) # averaged mse's: train and test
coefficients(lm(mpg~., data=AutoNum))

```
```{r}
table1=matrix(c("-1.453525e+01", "-3.298591e-01", "7.678430e-03", "-3.913556e-04", "-6.794618e-03", "8.527325e-02", "7.533672e-01"  ), 1, 7)
colnames(table1)=c("Intercept", "cylinders", "displacement", "horsepower", "weight", "acceleration", "year")
#rownames(table1)=c("Intercept","x2")
knitr::kable(table1, caption = "Coefficients of all predictors")
```


$MSE_{train}= 11.52738$ and $MSE_{test}= 12.02604$

***
*Part b: Forward Selection Method*
```{r results='hide'}
k=5
set.seed(99)
folds=sample(1:k,nrow(AutoNum),replace=TRUE)
best.fit=regsubsets(mpg~.,data=AutoNum[folds!=1,],
                    nvmax=6, method = "forward")
summary(best.fit)
cvtrain.errors=matrix(NA,k,6, dimnames=list(NULL, paste(1:6)))
cvtrain.errors
for(j in 1:k){
  best.fit=regsubsets(mpg~.,data=AutoNum[folds!=j,], nvmax=6, method = "forward")
  for(i in 1:6){
    pred=predict(best.fit,AutoNum[folds!=j,],id=i)
    cvtrain.errors[j,i]=mean((AutoNum$mpg[folds!=j]-pred)^2)
  }
}

cvtrain.errors
mean.cvtrain.errors=apply(cvtrain.errors,2,mean)
mean(mean.cvtrain.errors)
#mean(best.fit$rss/(392-(7+1)))   #MSE train = SSE/df
cv.errors=matrix(NA,k,6, dimnames=list(NULL, paste(1:6)))
cv.errors
for(j in 1:k){
  best.fit=regsubsets(mpg~.,data=AutoNum[folds!=j,], nvmax=6, method = "forward")
  for(i in 1:6){
    pred=predict(best.fit,AutoNum[folds==j,],id=i)
    cv.errors[j,i]=mean((AutoNum$mpg[folds==j]-pred)^2)
      }
}
cv.errors
mean.cv.errors=apply(cv.errors,2,mean)
mean(mean.cv.errors)
coef(best.fit, 2)

```
$MSE_{train} = 12.74865$  

The best model is model 2 with weight and year with $MSE_{test} = 13.04019$  
```{r}
table2=matrix(c("-14.140341838", "-0.006641689", "0.755087631"), 1, 3)
colnames(table2)=c("Intercept", "weight", "year")
knitr::kable(table2, caption = "Coefficients of model")
```


***
*Part c: Comparing test MSE's*  

We get $MSE_{OLStest}=12.02604$ and $MSE_{forwardtest} = 13.04019$. So for testing the least square method is better forward selection method performs. Normally we would expect subsetting to perform better but the small number of predictors semms to have affected our answer.

***

*Part d: Backward Selection Method* 
```{r results='hide'}
set.seed(99)
best.fit=regsubsets(mpg~.,data=AutoNum[folds!=1,],
                    nvmax=6, method = "backward")
summary(best.fit)
#mean(best.fit$rss/(392-(7+1)))   #MSE train = SSE/df
cvtrain.errors=matrix(NA,k,6, dimnames=list(NULL, paste(1:6)))
cvtrain.errors
for(j in 1:k){
  best.fit=regsubsets(mpg~.,data=AutoNum[folds!=j,], nvmax=6, method = "backward")
  for(i in 1:6){
    pred=predict(best.fit,AutoNum[folds!=j,],id=i)
    cvtrain.errors[j,i]=mean((AutoNum$mpg[folds!=j]-pred)^2)
  }
}
cvtrain.errors
mean.cvtrain.errors=apply(cvtrain.errors,2,mean)
mean(mean.cvtrain.errors)
cv.errors=matrix(NA,k,6, dimnames=list(NULL, paste(1:6)))
cv.errors
for(j in 1:k){
  best.fit=regsubsets(mpg~.,data=AutoNum[folds!=j,], nvmax=6, method = "backward")
  for(i in 1:6){
    pred=predict(best.fit,AutoNum[folds==j,],id=i)
    cv.errors[j,i]=mean((AutoNum$mpg[folds==j]-pred)^2)
  }
}
cv.errors
mean.cv.errors=apply(cv.errors,2,mean)
mean(mean.cv.errors)
coef(best.fit, 2)
```
$MSE_{train} = 12.74865$ and $MSE_{test}=13.04019$.    
The best model is model 2 which contains weight and year.   
```{r}
table3=matrix(c("-14.140341838", "-0.006641689", "0.755087631"), 1, 3)
colnames(table3)=c("Intercept", "weight", "year")
knitr::kable(table3, caption = "Coefficients of all predictors")
```

***
*Part e: Comparing parts b and d*  

We get the same answer in parts b and d, i.e. the forward and backward selection methods. This makes sense since our model only has 6 predictors so it's not computationally intensive. Also both methods use the same criteria of MSE and adjusted R-squared for selecting the "best" model so in our example it's okay to have the same conclusion.   

***
*Part f: Better fitted model*  

Even though the selection CV methods give a slightly higher test MSE, the forward/backward models reduce it to just 2 predictors with similar coefficients for weight and year. Based on this, subsets appear to be better models.
```{r}
knitr::kable(table1, caption = "OLS: Coefficients of all predictors")
knitr::kable(table2, caption = "Forward: Coefficients of all predictors")
knitr::kable(table3, caption = "Backward: Coefficients of all predictors")
```
***


\newpage


## Q3) Shrinkage Methods

*Part a: Fitting Ridge Regression*
```{r results='hide'}
library(glmnet)
x=model.matrix(mpg~.,AutoNum)[,-1]
dim(x) #392 6
y=AutoNum$mpg
hist(y)

# grid for lambda
grid=10^seq(10,-2,length=100)
grid
min(grid);max(grid)
# fit ridge regression: x is data or data matrix
ridge.mod=glmnet(x,y,alpha=0,lambda=grid) #alpha=1 is the lasso penalty, and alpha=0 the ridge penalty.


##
set.seed(99)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh = 1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])

mean((ridge.pred-y.test)^2) #test MSE
mean((mean(y[train])-y.test)^2)   #test set MSE

ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2)

ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train])
mean((ridge.pred-y.test)^2)

# fit mlr
lm(y~x, subset=train)

# get estimated/fitted values
predict(ridge.mod,s=0, exact=T,
        type="coefficients",x=x[train,],y=y[train])[1:7,]

# 
set.seed(99)
#?cv.glmnet #Does k-fold cross-validation for glmnet, produces a plot, and returns a value for lambda
cv.out=cv.glmnet(x,y,alpha=0, lambda=grid)
plot(cv.out)

bestlam=cv.out$lambda.min
cat(bestlam, "is the best lambda") # cross-validated lambda that yields best error

ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2) #test MSE associated with this best lambda above

# refit the model with optimal lambda (best)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:7,] 

```
lambda= 0.01747528      

$MSE=$ 12.71245  
```{r}
table4=matrix(c("-7.322075568", "-0.451557629", "-0.007147308", "-0.022745646", "-0.003990433", "-0.070426388",  "0.657821460"), 1,7)
colnames(table4)=c("Intercept", "cylinders", "displacement", "horsepower", "weight", "acceleration", "year")
knitr::kable(table4, caption="Ridge Regression: Predictors' coefficient estimates")
```

***
\newpage

*Part b: Fitting Lasso Regression*
```{r results='hide'}
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
set.seed(99)
cv.out=cv.glmnet(x,y,alpha=1)
plot(cv.out)

bestlam=cv.out$lambda.min
bestlam

lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2) #MSE

out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:7,]
lasso.coef  # see many coeffs are zero!! We did!

lasso.coef
lasso.coef[lasso.coef!=0]  


```
lambda=0.03889083 
$MSE=$ 12.58745  
```{r}
table5=matrix(c("-1.083322e+01","-9.018986e-02","0.000000e+00","-3.453417e-03","-6.152560e-03","2.050060e-05",  "7.035162e-01"), 1,7)
colnames(table5)=c("Intercept", "cylinders", "displacement", "horsepower", "weight", "acceleration", "year")
knitr::kable(table5, caption="Lasso: Predictors' coefficient estimates")
```

***
\newpage

*Part c: Comparing 3a & 3b to 1a*  
Comparing ordinary least squared regression with ridge and lasso regressions,   
- MSE is lower in OLS (11.591) than ridge (12.71) or lasso (12.58). Although the difference isn't huge
- As seen in the table below, coefficient for the displacement predictor is reduced from OLS to ridge, but lasso manages to eliminate it to 0. This is by efficient shrinking.
 

```{r}
table6=matrix(c("-1.453525e+01", "-3.298591e-01", "7.678430e-03", "-3.913556e-04", "-6.794618e-03", "8.527325e-02",  "7.533672e-01", "-7.322075568", "-0.451557629", "-0.007147308", "-0.022745646", "-0.003990433", "-0.070426388",  "0.657821460", "-1.083322e+01","-9.018986e-02","0.000000e+00","-3.453417e-03","-6.152560e-03","2.050060e-05",  "7.035162e-01"), 3,7)
colnames(table6)=c("Intercept", "cylinders", "displacement", "horsepower", "weight", "acceleration", "year")
rownames(table6)=c("LS, 1a", "Ridge, 3a", "Lasso, 3b")
knitr::kable(table6, caption="Comparing predictors' coefficient estimates")
```

***
\newpage
*Part d: 5-fold CV with OLS, Ridge and Lasso*  
OLS:
```{r results='hide'}
x=model.matrix(mpg~.,AutoNum)[,-1]
y=AutoNum$mpg
# grid for lambda
grid=10^seq(10,-2,length=100)
grid
min(grid);max(grid)
# fit ridge regression: x is data or data matrix
ridge.mod=glmnet(x,y,alpha=0,lambda=0) #alpha=1 is the lasso penalty, and alpha=0 the ridge penalty.
##
set.seed(99)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)

y.test=y[test]
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh = 1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])

olstestmse<-mean((ridge.pred-y.test)^2) #test MSE
mean((mean(y[train])-y.test)^2)   #test set MSE

ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2)

ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train])
olstrainmse<-mean((ridge.pred-y.test)^2)

# fit mlr
lm(y~x, subset=train)

# get estimated/fitted values
predict(ridge.mod,s=0, exact=T,
        type="coefficients",x=x[train,],y=y[train])[1:7,]

```
$MSE_{train}=$ `r olstrainmse`  

$MSE_{test}=$ `r olstestmse`
```{r}
tableols=matrix(c("-5.612184029", "-0.750934424", "0.007005015", "-0.016190276", "-0.005618296", "-0.159254236",  "0.694108227"), 1,7)
colnames(tableols)=c("Intercept", "cylinders", "displacement", "horsepower", "weight", "acceleration", "year")
knitr::kable(tableols, caption="5-fold CV OLS: Predictors' coefficient estimates")
```

RIDGE:
```{r results='hide'}
x=model.matrix(mpg~.,AutoNum)[,-1]
dim(x)
y=AutoNum$mpg
hist(y)

# grid for lambda
grid=10^seq(10,-2,length=100)
grid
min(grid);max(grid)
# fit ridge regression: x is data or data matrix
ridge.mod=glmnet(x,y,alpha=0,lambda=grid) 

##
set.seed(99)
train=sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid, thresh = 1e-12, nfolds=5)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])

mean((ridge.pred-y.test)^2) #test MSE
mean((mean(y[train])-y.test)^2)   #test set MSE

ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2)

ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train])
mean((ridge.pred-y.test)^2)

# fit mlr
lm(y~x, subset=train)

# get estimated/fitted values
predict(ridge.mod,s=0, exact=T,
        type="coefficients",x=x[train,],y=y[train])[1:7,]

# 
set.seed(99)
#?cv.glmnet #Does k-fold cross-validation for glmnet, produces a plot, and returns a value for lambda
cv.out=cv.glmnet(x[train,],y[train],alpha=0, nfolds = 5)
plot(cv.out)

bestlam=cv.out$lambda.min
cat(bestlam, "is the best lambda") # cross-validated lambda that yields best error

ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
ridgetrainmse<-mean((ridge.pred-y.test)^2) #test MSE associated with this best lambda above
ridgetestmse<-mean((mean(y[train])-y.test)^2)
# refit the model with optimal lambda (best)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:7,] 


```
$MSE_{train}=$ `r ridgetrainmse`
$MSE_{test}=$ `r ridgetestmse`


```{r}
tableridge=matrix(c("-7.322075568","-0.451557629", "-0.007147308", "-0.022745646", "-0.003990433", "-0.070426388",  "0.657821460"), 1,7)
colnames(tableridge)=c("Intercept", "cylinders", "displacement", "horsepower", "weight", "acceleration", "year")
knitr::kable(tableridge, caption="5-fold CV Ridge: Predictors' coefficient estimates")
```
LASSO:
```{r results='hide'}
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
set.seed(99)
cv.out=cv.glmnet(x[train,],y[train],alpha=1, nfolds = 5)
plot(cv.out)

bestlam=cv.out$lambda.min
bestlam

lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
lassotrainmse<-mean((lasso.pred-y.test)^2) #MSE
lassotestmse<-mean((mean(y[train])-y.test)^2)
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:7,]
lasso.coef  # see many coeffs are zero!! We did!

lasso.coef
lasso.coef[lasso.coef!=0]  


```
$MSE_{train}=$ `r lassotrainmse`
$MSE_{test}=$ `r lassotestmse` 

```{r}
tableridge=matrix(c("-10.569712264","-0.086794208","0.000000000", "-0.003439769","-0.006140150","0.000000000",   "0.699302632" ), 1,7)
colnames(tableridge)=c("Intercept", "cylinders", "displacement", "horsepower", "weight", "acceleration", "year")
knitr::kable(tableridge, caption="5-fold CV Lasso: Predictors' coefficient estimates")
```


***
*Part e: Comparing the tests for predicting mpg*  

When we consider the test mean squared errors for OLS, Ridge and Lasso regressions, the lasso method gives us the lowest MSE. However the difference between these MSEs is rather small.
For this dataset, lasso eliminates 2 predictors so seems to be the best method for predicting mpg.

***


\newpage

## Write comments, questions: ...


***
I hereby write and submit my solutions without violating the academic honesty and integrity. If not, I accept the consequences. 

### List the fiends you worked with (name, last name): Ravi Dugh

### Disclose the resources or persons if you get any help: Course material

### How long did the assignment work take?: Around 15 hours (interesting but long!)


***
## References
...
